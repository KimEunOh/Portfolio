{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\douly\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lmm-FvM8fsL0-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "import re\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 API 키 읽기\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# CLIP 모델과 프로세서 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 실행 흐름\n",
    "input_file = \"combined_fake.jsonl\"\n",
    "keywords_file = \"keyword.json\"\n",
    "embedding_file = \"keyword_embeddings.json\"\n",
    "rag_dataset_file = \"rag_dataset_v2.json\"\n",
    "username = \"KimEunOh\"\n",
    "repo = \"image\"\n",
    "branch = \"main\"\n",
    "folder_path = \"final_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. output_data로부터 assistant의 content값 추출\n",
    "def extract_assistant_content(input_file):\n",
    "    contents = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line.strip())\n",
    "            for message in data.get(\"messages\", []):\n",
    "                if message.get(\"role\") == \"assistant\":\n",
    "                    content = message.get(\"content\", \"\").strip()\n",
    "                    if content:\n",
    "                        contents.append(content)\n",
    "    return contents\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 마크다운, 불필요한 기호, 공백을 제거\n",
    "    \"\"\"\n",
    "    # 1. 마크다운 포맷 제거\n",
    "    text = re.sub(r\"\\*\\*|\\*|__|_|`\", \"\", text)  # Bold, Italic, Underline, Inline Code\n",
    "    text = re.sub(r\"#+\\s?\", \"\", text)  # Header (#)\n",
    "\n",
    "    # 2. 특수 문자 제거\n",
    "    text = re.sub(r\"[()\\[\\]{}]\", \"\", text)  # 괄호 및 대괄호\n",
    "    text = re.sub(r\"[<>]\", \"\", text)  # 꺽쇠 괄호\n",
    "\n",
    "    # 3. 다중 공백과 줄바꿈 처리\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 다중 공백 제거\n",
    "    text = text.strip()  # 양쪽 공백 제거\n",
    "\n",
    "    # 4. 빈 문자열 검사\n",
    "    if not text:\n",
    "        print(\"Warning: Text is empty after cleaning.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def truncate_text(text, max_length=1000):\n",
    "    \"\"\"\n",
    "    입력 텍스트 길이 조절.\n",
    "    \"\"\"\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_keywords_with_llm(contents, output_file):\n",
    "    \"\"\"\n",
    "    GPT를 사용하여 키워드를 추출하고 JSON 파일에 저장\n",
    "    \"\"\"\n",
    "    keywords_data = []\n",
    "    for idx, content in enumerate(contents):\n",
    "        cleaned_content = clean_text(content)\n",
    "        truncated_content = truncate_text(cleaned_content, max_length=1000)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI model trained to analyze text and extract deepfake indicators or clues. These clues can include unnatural features, inconsistencies, or anomalies commonly found in deepfake media.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                Analyze the following text to extract key deepfake clues. Focus on identifying abnormalities or inconsistencies in facial features, textures, lighting, or expressions that might suggest digital manipulation.\n",
    "\n",
    "                Text:\n",
    "                {truncated_content}\n",
    "\n",
    "                Extracted Clues:\n",
    "                1. Identify any unnatural features, such as misaligned eyes, uneven lighting, or irregular skin texture.\n",
    "                2. Include specific details about abnormalities, such as asymmetrical facial features, inconsistent reflections, or poorly rendered details like teeth or hair.\n",
    "                3. Mention lighting inconsistencies, shadow mismatches, or environmental factors that suggest editing.\n",
    "                4. List these clues as specific and concise keywords separated by commas.\n",
    "\n",
    "                Keywords:\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "        try:\n",
    "            gpt = ChatOpenAI(\n",
    "                temperature=0,\n",
    "                model_name=\"gpt-4o\",  # 모델명\n",
    "            )\n",
    "            response = gpt.invoke(messages)\n",
    "            keywords = response.content.strip()\n",
    "            print(f\"Keywords for item {idx}: {keywords}\")\n",
    "            keywords_data.append({\"content\": content, \"keywords\": keywords})\n",
    "        except KeyError as ke:\n",
    "            # 응답 형식이 예상과 다를 경우 오류 처리\n",
    "            print(f\"KeyError for item {idx}: {ke}\")\n",
    "            logging.error(f\"KeyError for item {idx}: {ke} - Response: {response}\")\n",
    "            keywords_data.append(\n",
    "                {\"content\": content, \"keywords\": \"default, keyword, placeholder\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # 기타 예외 처리\n",
    "            print(f\"Error extracting keywords for content at index {idx}: {e}\")\n",
    "            logging.error(f\"Error extracting keywords for content at index {idx}: {e}\")\n",
    "            keywords_data.append(\n",
    "                {\"content\": content, \"keywords\": \"default, keyword, placeholder\"}\n",
    "            )\n",
    "    # JSON 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(keywords_data, outfile, ensure_ascii=False, indent=4)\n",
    "    return keywords_data\n",
    "\n",
    "\n",
    "# 3. keyword.json을 CLIP를 통해 임베딩을 수행하고, DB에 저장\n",
    "def embed_keywords_with_clip(keyword_file, output_file):\n",
    "    with open(keyword_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        keywords_data = json.load(infile)\n",
    "\n",
    "    embeddings = []\n",
    "    for entry in keywords_data:\n",
    "        keywords = entry[\"keywords\"]\n",
    "        inputs = clip_processor(\n",
    "            text=[keywords], return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = clip_model.get_text_features(**inputs)\n",
    "        embeddings.append(\n",
    "            {\"keywords\": keywords, \"embedding\": text_embedding.cpu().tolist()}\n",
    "        )\n",
    "    # JSON 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(embeddings, outfile, ensure_ascii=False, indent=4)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "# 4. GitHub의 목록으로부터 이미지 URL 리스트 전달받음\n",
    "def get_github_image_urls(username, repo, branch, folder_path, file_names=None):\n",
    "    api_url = f\"https://api.github.com/repos/{username}/{repo}/contents/{folder_path}?ref={branch}\"\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()\n",
    "        image_urls = [\n",
    "            f\"https://raw.githubusercontent.com/{username}/{repo}/{branch}/{folder_path}/{file['name']}\"\n",
    "            for file in files\n",
    "            if file.get(\"name\", \"\").lower().endswith((\".jpg\", \".jpeg\", \".png\")) and (file_names is None or file['name'] in file_names)\n",
    "        ]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching image URLs from GitHub: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 5. 이미지 임베딩을 수행하여, 기존에 저장된 텍스트 임베딩 값과 유사도 계산\n",
    "def embed_image_and_calculate_similarity(image_urls, keyword_embeddings):\n",
    "    results = []\n",
    "    for image_url in image_urls:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = clip_model.get_image_features(**inputs).cpu()\n",
    "\n",
    "            # 유사도 계산\n",
    "            similarities = [\n",
    "                {\n",
    "                    \"keywords\": entry[\"keywords\"],\n",
    "                    \"similarity\": torch.nn.functional.cosine_similarity(\n",
    "                        torch.tensor(entry[\"embedding\"]), image_embedding\n",
    "                    ).item(),\n",
    "                }\n",
    "                for entry in keyword_embeddings\n",
    "            ]\n",
    "            most_relevant = max(similarities, key=lambda x: x[\"similarity\"])\n",
    "            results.append({\"image_url\": image_url, \"relevant_keywords\": most_relevant})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_url}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# 6. 추출된 키워드들을 바탕으로 프롬프트 생성\n",
    "def generate_prompts(results):\n",
    "    prompts = []\n",
    "    for result in results:\n",
    "        image_url = result[\"image_url\"]\n",
    "        relevant_keywords = [result[\"relevant_keywords\"][\"keywords\"]]\n",
    "        prompt = f\"\"\"\n",
    "            Analyze the image at the following URL: {image_url}.  \n",
    "            Using the following keywords as a checklist: {', '.join(relevant_keywords)}.  \n",
    "\n",
    "            Instructions:  \n",
    "            1. For each keyword, carefully examine the image and objectively verify whether the described clue is present. Avoid making assumptions and clearly state if no issues are observed.  \n",
    "            2. Maintain neutrality in your evaluation and do not assume the presence of manipulation unless there is strong and consistent evidence across multiple clues.  \n",
    "            3. If anomalies are observed, provide a factual description without inferring intent or confirming manipulation prematurely.  \n",
    "            4. If no clear anomalies are found, highlight natural variations or explain why observed features may align with expected real-world variations.  \n",
    "\n",
    "            Response Format:  \n",
    "            1. Observations: Provide an objective analysis for each keyword, explicitly stating whether the described clue matches observed details or not.  \n",
    "            2. Assessment: Summarize whether there is sufficient evidence to suggest manipulation, emphasizing neutrality and avoiding bias.  \n",
    "            3. Explanation: Clearly justify your observations and assessment, ensuring a balanced perspective that accounts for both natural variations and anomalies.\n",
    "\n",
    "            Example Response:  \n",
    "            Observations:  \n",
    "            - \"Mismatched shadows\": Shadows appear consistent with the lighting source.  \n",
    "            - \"Unnatural facial features\": Slight asymmetry observed, but within the range of natural human variation.  \n",
    "            - \"Smooth skin texture\": Minor smoothness noted, but could be due to lighting or image compression.  \n",
    "\n",
    "            Assessment: There is no strong evidence of manipulation in this image.  \n",
    "\n",
    "            Explanation: While slight irregularities were noted, these can be attributed to natural variations or external factors like lighting and resolution. No significant anomalies were found to indicate deepfake manipulation.\n",
    "            \"\"\"\n",
    "        prompts.append({\"image_url\": image_url, \"prompt\": prompt.strip()})\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# 7. 이미지 URL과 프롬프트를 쌍으로 한 rag_dataset.json 파일 생성\n",
    "def save_rag_dataset(prompts, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(prompts, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merged_Labels_and_Predictions.csv 파일 읽기\n",
    "df = pd.read_csv(\"Merged_Labels_and_Predictions.csv\")\n",
    "\n",
    "# prediction 값이 0인 리스트를 file_names 리스트로 생성\n",
    "file_names = df[df['predicted'] == 0]['FileName'].tolist()\n",
    "labels = df[df['predicted'] == 0]['Label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n새로운 output_data가 추가되었을 경우에만 실행 cntl + /\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "새로운 output_data가 추가되었을 경우에만 실행 cntl + /\n",
    "\"\"\"\n",
    "\n",
    "# # 단계별 실행\n",
    "\n",
    "# # 컨텐츠 추출\n",
    "# assistant_contents = extract_assistant_content(input_file)\n",
    "# # 키워드 생성\n",
    "# keywords_data = extract_keywords_with_llm(assistant_contents, keywords_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 임베딩\n",
    "keyword_embeddings = embed_keywords_with_clip(keywords_file, embedding_file)\n",
    "# 이미지 리스트 생성\n",
    "\n",
    "image_urls = get_github_image_urls(username, repo, branch, folder_path, file_names=file_names)\n",
    "\n",
    "# 이미지 임베딩 및 유사도 계산\n",
    "\n",
    "results = embed_image_and_calculate_similarity(image_urls, keyword_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG dataset saved to rag_dataset_v2.json\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 생성\n",
    "prompts = generate_prompts(results)\n",
    "save_rag_dataset(prompts, rag_dataset_file)\n",
    "\n",
    "print(f\"RAG dataset saved to {rag_dataset_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_names = [\n",
    "    \"003250.jpg\",\n",
    "    \"004925.jpg\",\n",
    "    \"008818.jpg\",\n",
    "    \"009318.jpg\",\n",
    "    \"009913.jpg\",\n",
    "    \"104932_1976-02-15_2012.jpg\",\n",
    "    \"12277_1937-12-30_2013.jpg\",\n",
    "    \"145712_1915-06-09_2008.jpg\",\n",
    "    \"149812_1973-07-15_2009.jpg\",\n",
    "    \"186477_1911-11-05_1940.jpg\",\n",
    "    \"239012_1982-05-22_2006.jpg\",\n",
    "    \"263977_1942-03-14_2015.jpg\",\n",
    "    \"289632_1971-02-13_2008.jpg\",\n",
    "    \"308212_1956-10-19_2011.jpg\",\n",
    "    \"318477_1911-06-26_1947.jpg\",\n",
    "    \"36732_1963-01-15_2013.jpg\",\n",
    "    \"375212_1949-05-18_2012.jpg\",\n",
    "    \"396732_1931-09-29_2010.jpg\",\n",
    "    \"442612_1967-04-01_2004.jpg\",\n",
    "    \"45812_1948-02-19_2002.jpg\",\n",
    "    \"499512_1941-06-02_2007.jpg\",\n",
    "    \"83312_1951-10-02_2007.jpg\",\n",
    "    \"8432_1947-07-03_2011.jpg\",\n",
    "]\n",
    "\n",
    "# rag_dataset_v2.json 파일 읽기\n",
    "with open('rag_dataset_v2.json', 'r') as f:\n",
    "    rag_data = json.load(f)\n",
    "\n",
    "# 레이블 추가\n",
    "for item in rag_data:\n",
    "    # image_url에서 파일명 추출\n",
    "    file_name = item['image_url'].split('/')[-1]  # URL에서 파일명 추출\n",
    "    # prompt에서 파일명 추출\n",
    "    # prompt = item['prompt']\n",
    "    # file_name_from_prompt = prompt.split(' ')[-1]  # 필요시 prompt에서 파일명 추출\n",
    "\n",
    "    # 문자열 포함 여부로 레이블 생성\n",
    "    item['label'] = 1 if any(fn in file_name for fn in file_names) else 0\n",
    "\n",
    "# 수정된 데이터 저장\n",
    "with open('rag_dataset_v2_test.json', 'w') as f:\n",
    "    json.dump(rag_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biztour-2MXTw1DY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
