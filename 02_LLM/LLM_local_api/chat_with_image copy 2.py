import os
from pathlib import Path
from typing import List, Optional, Dict, Generator, AsyncGenerator, Union, Tuple, Any
from fastapi import FastAPI, Request, Form
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, StreamingResponse
import uvicorn
import requests
import json
import time
from dotenv import load_dotenv
from retriever import retrieve_from_local_file
import base64
from PIL import Image
from io import BytesIO
import re
import asyncio
from prometheus_client import Gauge, Counter, Histogram, start_http_server
import psutil

# 예시: vLLM 서버에서 제공하는 특정 메트릭을 로컬 Prometheus로 옮기는 용도
VLLM_ACTIVE_SESSIONS = Gauge(
    "vllm_active_sessions", "Number of active sessions in vLLM server"
)
VLLM_TOTAL_TOKENS = Counter(
    "vllm_total_tokens_generated", "Total tokens generated by vLLM"
)

# 이미 정의된 REQUEST_COUNT, LATENCY_HIST 등이 있다고 가정
REQUEST_COUNT = Counter("vllm_request_count", "Number of requests processed")
LATENCY_HIST = Histogram(
    "vllm_inference_latency_seconds", "Inference latency histogram"
)

# 새로운 메트릭 정의
REQUEST_ERRORS = Counter("api_request_errors", "Number of API request errors")
TOKEN_GENERATION_SPEED = Gauge("token_generation_speed", "Tokens per second")

# 메트릭 주기적 계산
async def calculate_metrics_loop():
    """주기적으로 자체 메트릭을 계산하고 업데이트"""
    while True:
        try:
            # 시스템 메트릭 수집 (예: CPU, 메모리 사용량)
            process = psutil.Process(os.getpid())
            cpu_percent = process.cpu_percent()
            memory_info = process.memory_info()
            
            # 사용자 정의 메트릭에 반영
            SYSTEM_CPU_PERCENT.set(cpu_percent)
            SYSTEM_MEMORY_USAGE.set(memory_info.rss / 1024 / 1024)  # MB 단위
            
        except Exception as e:
            print(f"메트릭 계산 중 오류: {e}")
        
        await asyncio.sleep(15)

app = FastAPI()
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# 전역 변수로 봇 인스턴스 캐시
bot_instance = None

vllm_host = "https://4ucbikmtl97a44-8000.proxy.runpod.net"
vllm_url = f"{vllm_host}/generate"
vllm_metrics_url = f"{vllm_host}/metrics"


class ImageRAGChatBot:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.retriever = None
        self.system_message = """
        당신은 제공된 컨텍스트를 기반으로 정확한 답변을 제공하는 도우미입니다.
        제공된 컨텍스트에 질문과 관련된 정보가 있는 경우 컨텍스트에 있는 정보를 사용하여 질문에 답변하세요.
        일반적인 질문에는 컨텍스트에 있는 정보를 사용하지 않고 일반적인 응답을 제공하세요.
        컨텍스트에 관련 정보가 포함되어 있지 않은 경우 이를 인정하고 일반적인 응답을 제공하세요.
        컨텍스트에 이미지가 제공되면 이미지를 분석하고 응답에 내용을 포함시키세요.
        반드시 한국어로만 답변해야 합니다. 영어 사용은 금지됩니다.
        """

    def load_knowledge_base(self, folder_path: str) -> None:
        try:
            folder = Path(folder_path)
            if not folder.exists():
                raise FileNotFoundError(f"폴더를 찾을 수 없습니다: {folder_path}")

            pdf_files = list(folder.glob("*.pdf"))
            if not pdf_files:
                raise FileNotFoundError(f"폴더에 PDF 파일이 없습니다: {folder_path}")

            for pdf_file in pdf_files:
                print(f"Loading: {pdf_file.name}")
                if not self.retriever:
                    self.retriever = retrieve_from_local_file(str(pdf_file))
                else:
                    new_retriever = retrieve_from_local_file(str(pdf_file))
                    self.retriever.vectorstore.merge_from(new_retriever.vectorstore)

            print(f"Knowledge base loaded successfully from {folder_path}")
            print(f"Total loaded files: {len(pdf_files)}")

        except Exception as e:
            print(f"Error loading knowledge base: {str(e)}")

    def _get_relevant_context(self, query: str) -> Dict:
        if not self.retriever:
            return {"text": "", "images": []}

        try:
            # 최신 LangChain API와 호환되도록 업데이트
            try:
                # 새로운 invoke 메서드 사용 시도
                documents = self.retriever.invoke(query, config={"k": 3})
            except (AttributeError, TypeError):
                # 하위 호환성을 위해 이전 메서드 사용
                documents = self.retriever.get_relevant_documents(query, k=3)

            context_parts = []
            images = []

            for doc in documents:
                source = doc.metadata.get("source", "Unknown source")
                page = doc.metadata.get("page", "Unknown page")

                # 파일 경로에서 파일명만 추출
                file_name = (
                    source.split("/")[-1]
                    if "/" in source
                    else source.split("\\")[-1] if "\\" in source else source
                )

                # PDF에서 이미지 추출 로직 추가
                if hasattr(doc, "page_content") and hasattr(doc, "metadata"):
                    try:
                        # PDF 페이지에서 이미지 추출
                        pdf_images = self._extract_images_from_pdf(
                            doc.metadata.get("source", ""), doc.metadata.get("page", 1)
                        )
                        for img_data in pdf_images:
                            images.append(
                                {"data": img_data, "source": file_name, "page": page}
                            )
                    except Exception as e:
                        print(f"이미지 추출 오류: {str(e)}")

                context_parts.append(
                    f"[출처: {file_name}, 페이지: {page}]\n{doc.page_content}"
                )

            return {"text": "\n\n".join(context_parts), "images": images}
        except Exception as e:
            print(f"컨텍스트 검색 중 오류 발생: {str(e)}")
            return {"text": "", "images": []}

    def _extract_images_from_pdf(self, pdf_path: str, page_number: int) -> List[str]:
        """PDF 페이지에서 이미지를 추출하고 base64로 인코딩"""
        try:
            from pdf2image import convert_from_path

            images = convert_from_path(
                pdf_path, first_page=page_number, last_page=page_number
            )
            result = []
            for img in images:
                buffered = BytesIO()
                img.save(buffered, format="PNG")
                img_str = base64.b64encode(buffered.getvalue()).decode()
                result.append(img_str)
            return result
        except Exception as e:
            print(f"PDF 이미지 추출 오류: {str(e)}")
            return []

    def _clean_text(self, text: str, full_cleaning: bool = False) -> str:
        """텍스트 정리를 위한 공통 함수"""
        try:
            if not isinstance(text, str):
                if isinstance(text, list):
                    text = "".join([str(item) for item in text])
                else:
                    text = str(text) if text is not None else ""

            # 기본 정리 작업
            text = re.sub(r"\[INST\].*?\[/INST\]", "", text, flags=re.DOTALL)
            text = text.replace("<s>", "").replace("</s>", "")
            text = re.sub(r"\s*\[/INST\]\s*$", "", text)
            text = text.strip()

            # 전체 응답 정리가 필요한 경우 추가 작업 수행
            if full_cleaning:
                # 컨텍스트 정보 제거
                text = re.sub(
                    r"다음은.*?정보입니다:.*?(?=\n\n|$)", "", text, flags=re.DOTALL
                )
                # 이미지 설명 제거
                text = re.sub(r"이미지 \d+:.*?(?=\n\n|$)", "", text, flags=re.DOTALL)
                # 출처 인용 형식 정리
                text = re.sub(r"\[출처:.*?페이지:.*?\]", "", text)
                # 불필요한 공백 및 줄바꿈 정리
                text = re.sub(r"\n\s*\n", "\n\n", text)
                text = re.sub(r"\n{3,}", "\n\n", text)
                text = re.sub(r"^\s+", "", text, flags=re.MULTILINE)
                text = re.sub(r"\s+$", "", text, flags=re.MULTILINE)
                text = re.sub(r"\s+", " ", text)

            return text
        except Exception as e:
            print(f"텍스트 정리 중 오류 발생: {str(e)}")
            return str(text) if text else ""

    def _process_stream_chunk(self, chunk: str) -> str:
        """스트리밍 청크 처리 및 정리"""
        return self._clean_text(chunk)

    def _clean_response_text(self, text: str) -> str:
        """전체 응답 텍스트 정리"""
        return self._clean_text(text, full_cleaning=True)

    def _prepare_prompt_data(self, prompt: str) -> Tuple[Dict, Dict, List[Dict]]:
        """프롬프트 처리 및 컨텍스트 데이터 준비를 위한 공통 함수"""
            context_data = self._get_relevant_context(prompt)
            augmented_prompt = self._create_augmented_prompt(prompt, context_data)

            # 이미지 처리를 위한 텍스트 설명 추가
            image_descriptions = []
            if context_data["images"]:
                for i, img in enumerate(context_data["images"]):
                    image_descriptions.append(
                        f"이미지 {i+1}: 문서 {img['source']}, 페이지 {img['page']}의 이미지가 있습니다."
                    )

            image_text = "\n".join(image_descriptions)
        prompt_with_images = (
            f"{augmented_prompt['text']}\n\n{image_text}"
            if image_text
            else augmented_prompt["text"]
        )

        # API 요청을 위한 페이로드 생성
            payload = {
                "prompt": f"<s>[INST] {self.system_message} [/INST]\n\n[INST] {prompt_with_images} [/INST]",
                "temperature": 0.0,
                "max_tokens": 1024,
            "stop": ["<|eot_id|>", "<|eom_id|>", "[/INST]"],
        }

        sources = self._extract_sources(context_data["text"])

        return context_data, payload, sources

    def _extract_response_text(self, data: Dict) -> str:
        """다양한 API 응답 형식에서 텍스트 추출"""
        if "text" in data:
            return data["text"]
        elif "outputs" in data and len(data["outputs"]) > 0:
            output = data["outputs"][0]
            if isinstance(output, dict) and "text" in output:
                return output["text"]
            elif isinstance(output, str):
                return output
        elif "choices" in data and len(data["choices"]) > 0:
            choice = data["choices"][0]
            if isinstance(choice, dict):
                if "text" in choice:
                    return choice["text"]
                elif "delta" in choice and "content" in choice["delta"]:
                    return choice["delta"]["content"]
        return ""

    def _create_fallback_response(
        self, context_data: Dict, sources: List[Dict], error_msg: Optional[str] = None
    ) -> Dict:
        """오류 발생 시 기본 응답 생성"""
        fallback_text = "현재 AI 모델에 연결할 수 없습니다. 다음은 관련 문서에서 찾은 정보입니다:\n\n"
        if context_data.get("text"):
            fallback_text += context_data["text"]
        else:
            fallback_text += "관련 정보를 찾지 못했습니다."

        if error_msg:
            print(f"오류 발생: {error_msg}")

        return {
            "text": fallback_text,
            "full_text": fallback_text,
            "images": context_data.get("images", []),
            "sources": sources,
            "done": True,
        }

    def get_streaming_response(self, prompt: str) -> Generator[Dict, None, None]:
        """스트리밍 응답을 제공하는 제너레이터 함수"""
        REQUEST_COUNT.inc()
        start_t = time.time()

        try:
            # 공통 프롬프트 데이터 준비
            context_data, payload, sources = self._prepare_prompt_data(prompt)

            # 스트리밍 모드 활성화
            payload["stream"] = True

            # 응답 누적을 위한 변수
            full_response = ""
            previously_processed_text = ""

            try:
                # 서버 API가 지원하는 스트리밍 응답 형식에 따라 처리
                print("스트리밍 응답 처리 시작")

                stream_start = time.time()

                with requests.post(
                    vllm_url, json=payload, timeout=60, stream=True
                ) as response:
                    response.raise_for_status()  # HTTP 오류 확인

                    # 응답 형식 확인을 위한 디버깅 로그
                    content_type = response.headers.get("content-type", "")
                    print(f"응답 Content-Type: {content_type}")

                    for line in response.iter_lines():
                        if not line:
                            continue

                        # 바이트를 문자열로 디코딩
                        try:
                            line_text = line.decode("utf-8")
                        except UnicodeDecodeError as e:
                            print(f"디코딩 오류: {e}")
                            continue

                        # 디버깅을 위한 원시 라인 출력
                        print(f"원시 응답 라인: {line_text[:50]}...")

                        # SSE 형식 처리 ('data:' 접두어)
                        if line_text.startswith("data:"):
                            line_text = line_text[5:].strip()

                        # [DONE] 메시지 처리
                        if line_text == "[DONE]":
                            print("스트림 종료 ([DONE] 수신)")
                            continue

                        # 빈 라인 스킵
                        if not line_text:
                            continue

                        try:
                            # JSON 파싱
                            data = json.loads(line_text)

                            # 다양한 응답 형식에서 텍스트 추출
                            chunk_text = self._extract_response_text(data)

                            if isinstance(chunk_text, list):
                                chunk_text = "".join(str(item) for item in chunk_text)

                            # 청크가 없으면 다음 반복으로
                            if not chunk_text:
                                print("청크에 텍스트가 없음, 스킵")
                                continue

                            # 중복 청크 방지를 위한 차이점 추출
                            if (
                                chunk_text.startswith(previously_processed_text)
                                and len(previously_processed_text) > 0
                            ):
                                # 이전에 처리한 부분을 제외하고 새 부분만 사용
                                new_chunk = chunk_text[len(previously_processed_text) :]
                                previously_processed_text = chunk_text  # 전체 업데이트
                            else:
                                # 완전히 새로운 청크
                                new_chunk = chunk_text
                                previously_processed_text = chunk_text

                            # 빈 청크 스킵
                            if not new_chunk.strip():
                                continue

                            # 청크 정리
                            cleaned_chunk = self._process_stream_chunk(new_chunk)

                            # 응답 누적
                            full_response += cleaned_chunk

                            # 응답 객체 생성 및 반환
                            yield {
                                "text": cleaned_chunk,
                                "full_text": full_response,
                                "images": context_data["images"],
                                "sources": sources,
                                "done": False,
                            }
                        except json.JSONDecodeError as je:
                            print(
                                f"JSON 디코딩 오류: {je}, 원시 데이터: {line_text[:50]}..."
                            )
                            continue
                        except Exception as e:
                            print(f"청크 처리 중 오류: {str(e)}")
                            continue

                # 스트리밍 종료 - 최종 응답 반환
                stream_end = time.time()
                print(f"스트리밍 처리 시간: {stream_end - stream_start:.2f}초")
                yield {
                    "text": "",
                    "full_text": full_response,
                    "images": context_data["images"],
                    "sources": sources,
                    "done": True,
                }

            except requests.exceptions.RequestException as e:
                # 오류 발생 시 기본 응답 생성 및 반환
                yield self._create_fallback_response(context_data, sources, str(e))

            # 지연 시간 측정
            LATENCY_HIST.observe(time.time() - start_t)

        except Exception as e:
            error_msg = f"스트리밍 응답 생성 중 오류 발생: {str(e)}"
            print(error_msg)
            yield {
                "text": error_msg,
                "full_text": error_msg,
                "images": [],
                "sources": [],
                "done": True,
            }

    def get_response(
        self, prompt: str, stream: bool = False
    ) -> Union[Generator[Dict, None, None], Dict]:
        """프롬프트에 대한 응답을 생성 (스트림 또는 일반 모드)"""

        REQUEST_COUNT.inc()
        # (실제 지연 시간 측정을 위해) 모니터링 시작
        start_t = time.time()
        if stream:
            # 스트리밍 모드에서는 제너레이터 반환
            return self.get_streaming_response(prompt)

        try:
            # 공통 프롬프트 데이터 준비
            context_data, payload, sources = self._prepare_prompt_data(prompt)

            try:
                # vllm API 호출
                response = requests.post(vllm_url, json=payload, timeout=60)
                data = json.loads(response.content)

                # 응답 텍스트 추출
                raw_response_text = self._extract_response_text(data)
                if not raw_response_text:
                    raw_response_text = "응답을 받을 수 없습니다."

                # 응답 텍스트 정리
                cleaned_response = self._clean_response_text(raw_response_text)

                # 응답 구조화
                return {
                    "text": cleaned_response,
                    "images": context_data["images"],
                    "sources": sources,
                }
            except requests.exceptions.RequestException as e:
                # 오류 발생 시 기본 응답 반환
                fallback = self._create_fallback_response(context_data, sources, str(e))
                return {
                    "text": fallback["full_text"],
                    "images": fallback["images"],
                    "sources": fallback["sources"],
                }
        except Exception as e:
            error_msg = f"응답 생성 중 오류 발생: {str(e)}"
            print(error_msg)
            return {"text": error_msg, "images": [], "sources": []}

    def _create_augmented_prompt(self, prompt: str, context_data: Dict) -> Dict:
        if context_data["text"]:
            return {
                "text": f"""
                다음은 여러 문서에서 가져온 컨텍스트 정보입니다:
                {context_data['text']}

                위의 컨텍스트와 제공된 이미지를 바탕으로 다음 질문에 답변해주세요:
                {prompt}
                
                컨텍스트에서 정보를 제공할 때 출처를 인용해주세요.
                반드시 한국어로만 응답해주세요.
                """
            }
        return {"text": prompt}

    def _extract_sources(self, context_text: str) -> List[Dict]:
        """컨텍스트에서 출처 정보를 추출합니다."""
        sources = []
        try:
            # [출처: 파일명, 페이지: 번호] 형식의 패턴 매칭
            pattern = r"\[출처: (.*?), 페이지: (.*?)\]"
            matches = re.finditer(pattern, context_text)

            for match in matches:
                source = {"file": match.group(1), "page": match.group(2)}
                if source not in sources:  # 중복 제거
                    sources.append(source)
        except Exception as e:
            print(f"출처 추출 중 오류 발생: {str(e)}")
        return sources


# 봇 인스턴스 초기화 함수
def initialize_bot(force_reload=False):
    """봇 인스턴스를 초기화하는 함수"""
    global bot_instance

    if bot_instance is None or force_reload:
        try:
            load_dotenv()
            bot_instance = ImageRAGChatBot()
            documents_path = "documents"
            bot_instance.load_knowledge_base(documents_path)
            print("봇 인스턴스 생성 및 지식 베이스 로딩 완료")
        except Exception as e:
            print(f"초기화 중 오류 발생: {str(e)}")

    return bot_instance


# FastAPI 라우트
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


@app.post("/chat")
async def chat(prompt: str = Form(...), stream: bool = Form(False)):
    # 봇 인스턴스 초기화
    initialize_bot()

    if stream:
        # 스트리밍 응답을 위한 비동기 제너레이터
        async def stream_response():
            generator = bot_instance.get_response(prompt, stream=True)
            for chunk in generator:
                yield f"data: {json.dumps(chunk)}\n\n"

        return StreamingResponse(stream_response(), media_type="text/event-stream")
    else:
        # 일반 응답
        response = bot_instance.get_response(prompt, stream=False)
    return response


# 서버 시작 시 초기화 함수 추가
@app.on_event("startup")
async def startup_event():
    global prometheus_server
    
    # 봇 인스턴스 초기화
    initialize_bot(force_reload=True)
    
    # vLLM 메트릭 스크래핑 설정 (환경변수로 제어)
    enable_vllm_metrics = os.getenv("ENABLE_VLLM_METRICS", "false").lower() == "true"
    
    # 메트릭 스크래핑이 활성화된 경우에만 태스크 시작
    if enable_vllm_metrics:
        loop = asyncio.get_running_loop()
        loop.create_task(vllm_scrape_loop())
    else:
        print("vLLM 메트릭 스크래핑이 비활성화되었습니다.")
    
    # Prometheus 서버 설정 및 시작
    # ...


# -----------------------------------------------------------
# vLLM 서버 메트릭 주기적 스크레이핑 로직 (예시)
# -----------------------------------------------------------


async def vllm_scrape_loop():
    """vLLM 메트릭 스크래핑 루프 (옵션)"""
    # 먼저 메트릭 엔드포인트 존재 여부 확인
    try:
        resp = requests.get(vllm_metrics_url, timeout=5)
        if resp.status_code != 200:
            print(f"vLLM 메트릭 엔드포인트가 응답하지 않습니다 (코드: {resp.status_code}). 스크래핑을 중단합니다.")
            return  # 엔드포인트가 없으면 함수 종료
    except Exception as e:
        print(f"vLLM 메트릭 엔드포인트 연결 실패: {e}. 스크래핑을 중단합니다.")
        return
    
    # 이후 정상적인 스크래핑 루프 실행
    while True:
        try:
            resp = requests.get(vllm_metrics_url, timeout=5)
            if resp.status_code == 200:
                metrics_text = resp.text
                # vLLM 메트릭 포맷(plaintext)을 라인 단위로 분석
                # 아래는 예시로 "vllm_active_sessions" 또는 "vllm_total_tokens_generated" 같은 이름을 찾아서
                # 값만 파싱해 로컬에 업데이트하는 예시입니다.

                for line in metrics_text.split("\n"):
                    # 예: vllm_active_sessions 12
                    if line.startswith("vllm_active_sessions"):
                        try:
                            parts = line.split()
                            val = float(parts[1])
                            VLLM_ACTIVE_SESSIONS.set(val)
                        except:
                            continue

                    # 예: vllm_total_tokens_generated 12345
                    if line.startswith("vllm_total_tokens_generated"):
                        try:
                            parts = line.split()
                            val = float(parts[1])
                            # Counter는 set이 아닌 inc(증가) 개념이므로
                            # vLLM 측 total_tokens를 계속 누적한다고 가정
                            # 만약 누적값이 아니라 순간값이면 로직을 달리해야 함
                            VLLM_TOTAL_TOKENS.inc(val)
                        except:
                            continue

            else:
                print(f"[vllm_scrape_loop] vLLM 메트릭 요청 실패: {resp.status_code}")
        except Exception as e:
            print(f"[vllm_scrape_loop] 예외 발생: {e}")

        # 10초에 한 번씩 스크레이핑
        await asyncio.sleep(10)


# vLLM API 호출 시 직접 메트릭 수집
async def call_vllm_api(payload):
    start_time = time.time()
    try:
        response = requests.post(vllm_url, json=payload, timeout=60)
        response.raise_for_status()
        
        # 응답 시간 측정
        request_time = time.time() - start_time
        LATENCY_HIST.observe(request_time)
        
        # 토큰 수 추정 (응답 텍스트 기반)
        text = response.json().get("text", "")
        estimated_tokens = len(text.split())
        VLLM_TOTAL_TOKENS.inc(estimated_tokens)
        
        # 활성 세션 카운터 (단순 증가)
        VLLM_ACTIVE_SESSIONS.inc()
        
        return response.json()
    except Exception as e:
        # 요청 실패 시 메트릭 기록
        REQUEST_ERRORS.inc()  # 새로운 카운터 추가 필요
        raise e


if __name__ == "__main__":
    # Prometheus를 통해 해당 FastAPI 애플리케이션의 메트릭을 8080포트에서 노출
    start_http_server(8080)
    uvicorn.run(app, host="0.0.0.0", port=8000)
